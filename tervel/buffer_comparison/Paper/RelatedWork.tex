% !TEX root = WaitFreeRingBuffer.tex

In this section we describe the implementation of other concurrent multiple producers and multiple consumers ring buffers.
%Though we are aware of several single producer and/or single consumer designs we do not discuss them.
%@Steven, it wont be considered bizzare having a single line introduction??

Tsigas~\cite{tsigas_queue} presents a lock free ring buffer in which threads compete to update the head and tail locations.
This design achieves dead-lock and live-lock freedom, but as we show in Sec.~\ref{sec:results}, this design scales poorly.
We attribute this poor performance to contention placed on head and tail locations.
The threads that fail their \op{CAS} operations are then forced to retry at the next location.
Some contention is reduced by lazily updating the head and tail references.
This requires an increased amount of read operations for threads to locate the actual index to operate, offsetting some contention.
%In our design we are able to reduce contention by forcing threads to update the head and tail values before performing their operation.
%@Andrew, I think it will be better to to do the comparison in Algorithm overview section
%@Steven, what comparison? I only contrast
%Fortunately, the \op{faa} operation used for this is both effecient and scalable~\cite{feldman_vector}.
%@Andrew, the above sentence is out of place and does not transition well. Look below for my example and either provide a better sentence or use it.

-%you can say: "This design achieves dead-lock and live-lock freedom, but the nature by which threads perform enqueueue and dequeue operations lead to poor performance in scenarios of high thread contention.
-%We attribute this poor performance to how the design only allows a single thread to succeed at completing an enqueue or dequeue operation, forcing other threads to retry."
-%Ending  like this, provides a good tie into the next FAA approach
%@Steven, are you saying cut the rest (remove starting at "Some contention is reduced..")? 

Tsigas~\cite{tsigas_queue} presents a lock free ring buffer in which threads compete to update the head and tail locations.
Enqueue is performed by determining the tail of the buffer and then enqueuing an element using a \op{cas} operation.
Dequeue is performed by determining the head of the buffer and then dequeueing the current element using a \op{cas} operation. 
This design achieves dead-lock and live-lock freedom, however, if a thread is unable to perform a successful \op{cas} operation, the thread will starve.
Unlike other designs, which are able to diffuse contention, the competitive nature of this design leads to increased contention and poor scaling.

Krizhanovsky~\cite{krizhanovsky_queue} presents a lock-free and high performance ring buffer.
This implementation, relies on the \op{faa} operation to increment head and tail counters, which assigns the location to perform an operation, thereby reducing thread contention and providing better scaling.
Each thread maintains a separate tail and head value of the index it last completed an enqueue or dequeue, respectively. 
The smallest of all threads' local head and tail values are used to determine the head and tail value at which all threads have completed their operations.
These values prevent operations from attempting to enqueue or dequeue at a location the previous operation has not yet completed.
Though this a non-blocking design, it is blocking in the case that if the buffer is empty or full, threads performing a dequeue or enqueue will wait until the buffer is not empty or not full, respectively.
%@Andrew say something about it not being thread death safe

The industry standard for many concurrent data structures is provided by Intel Thread Building Blocks (TBB)~\cite{tbb}.
This library provides a \emph{concurrent bounded queue} which utilizes a fine-grained locking scheme.
The algorithm uses an array of \emph{micro queue} to alleviate contention on individual indices.
Upon starting an operation, threads are assigned a ticket value which is used to determine sequence of operations in each \emph{micro queue}.
%@Andrew How are elements added to micro queues
If a threads ticket is greater than the expected, it will spin wait until the delayed threads have completed their operations.


%Optional Conclusion: (telling the reader what he was just told) if the next section needs a tie in
%@Andrew: Do a short compare/contrast to each
%For Tsigas: Like Krizzhanovsky's buffer we use the FAA to achieve better scalling and to reduce thread contnention
%Then say however in contrast to krizhanovsky, we are able to avoid the danger of live-lock, through how we manage our sequence number.
%Then say our design is composable with micro queue structures, and  then specify why it is different to a micro queue.
%This should be one flowing paragraph that shows progression from one to the next


