% !TEX root = WaitFreeRingBuffer.tex
The ubiquitous use of ring buffers in computer applications make them a focal point in computer science research.
A ring buffer or cyclical queue is a first-in-first-out queue that stores elements on a fixed length array.
In contrast to linked list queues, which do not have a maximum capacity, ring buffers are limited in capacity by the length of this array.
This array allows for efficient O(1) operations, cache-aware optimizations, and low memory overhead.
In general, the memory utilization of a ring buffer is limited to the cost of the array and two counters, making it desirable for systems with limited memory resources, such as embedded systems.

Ring buffers are found in a wide range of applications such as network routing, multimedia processing, and cloud-based services.
Many of these applications depend on ring buffers to pass work from one thread to another.
With the rise in many-core architectures, the number of threads executing in a system has greatly increased.
As a result, the efficiency of shared data structures, such as ring buffers, have a greater impact on performance than ever before.

In an attempt to achieve efficient and scalable performance, there has been significant research into the development of non-blocking algorithms.
These non-blocking algorithms forgo the use of locks, to permit greater scalability and core utilization.
Such designs are categorized by the level of progress they guarantee, with wait-free as the highest and most desirable categorization.
It provides freedom from deadlock, livelock, and thread starvation.
Deadlock is a result of multiple operations waiting on the other to finish, thus blocking all operations involved in the deadlock.
Livelock is similar to deadlock except operations yield to each other causing a lack of progress from the threads repeatedly making attempts to allow the other operation to progress.
Starvation occurs when an operation waits indefinitely for a resource~\cite{herlihy_bible}.
In addition to wait-free, non-blocking designs can also be categorized as either lock-free or obstruction-free.
In contrast to wait-free, lock-free designs are susceptible to thread starvation and obstruction-free designs are susceptible to both livelock and thread starvation~\cite{herlihy_bible}.

We are aware of two other non-blocking ring buffers in literatue.
Tsigas et al. presented a lock-free approach in which threads compete to apply an operation~\cite{tsigas}, however, this approach suffers from thread congestion and poor scaling.
Krizhanovsky presented a non-blocking approach which improves scalability through the use of the \emph{fetch and add} operation~\cite{?}, but unfortunately the design is susceptible to thread starvation and is not thread death safe.


We propose a new wait-free ring buffer algorithm based on the atomic \textit{fetch and add} (\emph{FAA}) operation.
A thread performing an enqueue or dequeue operation will perform a \emph{FAA} on the tail or head sequence counter, respectively.
The returned sequence identifier (\emph{seqid}) is used to determine the position to enqueue or dequeue an element from the buffer.
Specifically, the position is determined by the \emph{seqid} modulo the buffer's length.
To address the case where the \emph{seqid} of two different threads refer to the same position, we employ a novel method of determining which thread is assigned the position and which thread must get a new position.
To prevent the case where a thread is continually prevented from completing its operation, we integrated a progress assurance scheme~\cite{feldman_vector}.



The thread performing a dequeue operation will attempt to replace an element node whose \emph{seqid} matches its assigned \emph{seqid} with a newly allocated empty node.
This empty node's \emph{seqid} is equal to the current \emph{seqid} plus the length of the buffer.
Our restriction on the type of node a dequeue thread may remove, allows us to derive a FIFO\footnote{First in First Out} ordering of elements.
Without this restriction it is \textit{theoretically} possible for a thread to enqueue $A$ and $B$ and then dequeue $B$ followed by $A$, breaking the FIFO property.
We explain the specific scenario which may lead to this in Sec.~\ref{sec:todo}.
However, this restriction also introduces the scenario where if a thread never dequeues its node, then other threads with higher sequence numbers must wait.
If this scenario is detected, an atomic bitmark is placed on that value.
By using this bitmark, we can allow a dequeuer to get a new \emph{seqid} without the risk of an element being enqueued with the old \emph{seqid}.


Enqueueing threads will attempt to replace the current value with an element node, if the current value is an empty node whose \emph{seqid} is equal to the thread's \emph{seqid}.
If the current value has a \emph{seqid} less than the thread's \emph{seqid}, the thread will perform a back off routine to provide a delayed thread the opportunity to complete its operation.
If the current value has not changed, then depending on the node type (element or empty) and the \emph{seqid} (less than or greater), the thread will either get a new \emph{seqid} or replace the current node with its element node.
We explore the various states and their associated actions in Sec.~\ref{sec:todo}.

Previous research has shown that the use of the \emph{FAA} operation can provide significant increase in performance, when compared to similar designs implemented with the atomic compare-and-swap operation (\emph{CAS}).
For example, Feldman et al.~\cite{feldman_vector} compared the performance a \emph{FAA} based vector pushBack operation and a \emph{CAS} based approach and found that the \emph{FAA} design outperforms the \emph{CAS} design by a factor of 2.3.
This work includes a naive approach to designing a wait-free ring buffer using a multi-word compare-and-swap algorithm (MCAS)~\cite{feldman_mcas}.
This allows us to explore the performance difference between our \emph{FAA} based approach to that of a \emph{CAS} based approach.
Sec.~\ref{sec:mcasbuffer} provides a detailed analysis of the performance difference between these approaches.

We compare the performance of our implementation to that of other known concurrent ring buffers.
In this comparison, we explore how different distributions of operations, number of threads, and ring buffer size affect the throughput of each implementation.
On average, our design outperforms other designs by \% operations per second.
Compared to Intel Thread Building Blocks' concurrent bounded queue, we perform \% more operations per second.
Our results support our hypothesis that our design is scalable, making it optimal for many-core and real-time systems.

We provide the following novel contributions:
\begin{itemize}
\item To our knowledge this the first wait-free ring buffer.
Other known approaches, are susceptible to hazards such as live-lock and thread starvation.

\item Our design presents a unique way of applying sequence numbers and bitmarking to maintain provide the FIFO property.
This allows a simple way to mark and correct out-of-sync locations.


\item Our design maintains throughput in scenarios of high thread contention.
Other known approaches degrade as the thread contention increases, making it ideal for highly parallel environments.


\end{itemize}